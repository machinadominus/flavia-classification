{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Now to mount drive here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923vIfGWXLul",
        "outputId": "d3fb9a11-c3fd-4e14-853b-8f7d5547ffe4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#after mounting drive we need to unzip the folder and use it\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/archive(1).zip'\n",
        "target_directory = '/content'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_directory)"
      ],
      "metadata": {
        "id": "zLN7wuM-3iEQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print('Found GPU. Using GPU.')\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "else:\n",
        "    try:\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print('Found TPU. Using TPU.')\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print('No GPU or TPU found. Using CPU.')\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "\n",
        "print('Number of replicas: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijw_alWhYQdl",
        "outputId": "555bdc5a-64e4-4e7d-f092-69896f86161d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU. Using GPU.\n",
            "Number of devices: 1\n",
            "Number of replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Read the image\n",
        "    original_image = cv2.imread(image_path)\n",
        "    original_image = cv2.resize(original_image, (224, 224))\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply threshold to obtain a binary mask for the white background\n",
        "    _, mask = cv2.threshold(gray_image, 200, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Invert the mask to get the foreground\n",
        "    mask = cv2.bitwise_not(mask)\n",
        "\n",
        "    # Apply Gaussian Blur to the image\n",
        "    blurred_image = cv2.GaussianBlur(original_image, (15, 15), 0)\n",
        "\n",
        "    # Sharpen the image\n",
        "    sharpened_image = cv2.addWeighted(blurred_image, 1.5, original_image, -0.5, 0)\n",
        "\n",
        "    # Combine the sharpened image with the white background mask\n",
        "    result_image = cv2.bitwise_and(sharpened_image, sharpened_image, mask=mask)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "DWGSV0BeYc5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_directory = '/content/Leaves'\n",
        "output_directory = '/content/Leaves_no_bg'\n",
        "csv_file = '/content/Leaves/all.csv'\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "with strategy.scope():\n",
        "  for index, row in df.iterrows():\n",
        "      image_name = row['id']\n",
        "      class_number = row['y']\n",
        "      class_directory = os.path.join(output_directory, str(class_number))\n",
        "      os.makedirs(class_directory, exist_ok=True)\n",
        "      input_path = os.path.join(input_directory, image_name)\n",
        "      output_path = os.path.join(class_directory, image_name)\n",
        "      processed_image = preprocess_image(input_path)\n",
        "      cv2.imwrite(output_path, processed_image)\n",
        "\n",
        "print(\"Images organized into class-specific folders after background removal.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PH1M7eOYOYi",
        "outputId": "ad67215c-f8d2-4f7a-b22d-53ad5f411bc0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images organized into class-specific folders after background removal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "transform = transforms.Compose([  # Convert to PIL Image\n",
        "    transforms.Resize((224, 224)),  # Resize to desired size\n",
        "  # Use your custom FeatureExtractor\n",
        "    transforms.ToTensor(),  # Convert back to PyTorch tensor\n",
        "])"
      ],
      "metadata": {
        "id": "pcM5sL4_XowT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_path = \"/content/Leaves_no_bg\"\n",
        "\n",
        "# Create a dataset from the folder structure\n",
        "dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "onEijgopXfFK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "-jR0lqdoZesQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pmMjODVZI37",
        "outputId": "a4228956-37ef-480f-b237-d7a3f857d1ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 32\n",
        "model.classifier = nn.Linear(model.config.hidden_size, num_classes)"
      ],
      "metadata": {
        "id": "BCSfihhMZnLc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxJtDndZqnW",
        "outputId": "63d603a9-f47b-48dc-dd2a-c7ad73fb22ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pGCVByTJZt47"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Free up GPU memory\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Free up GPU memory\n",
        "            del images, labels, outputs\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6kWjWL05B_J",
        "outputId": "fe732ebc-5492-4e3c-cfdb-5b6995bf7e0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 2.2048, Accuracy: 95.29%\n",
            "Epoch 2/5, Loss: 1.7821, Accuracy: 97.12%\n",
            "Epoch 3/5, Loss: 1.4766, Accuracy: 98.95%\n",
            "Epoch 4/5, Loss: 1.2532, Accuracy: 99.48%\n",
            "Epoch 5/5, Loss: 1.0695, Accuracy: 99.74%\n"
          ]
        }
      ]
    }
  ]
}